{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474fa788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e81cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "# disable some of the tf/keras training warnings \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "# suppress untraced functions warning\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54eab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (22.10.26)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.19.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.23.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (9.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tomar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed71bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained pose estimation model from Google Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Supported Mediapipe visualization tools\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from webcam footage\n",
    "    \n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ea4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff4b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # camera object\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # webcam video frame height\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # webcam video frame width\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS)) # webcam video fram rate \n",
    "\n",
    "# Set and test mediapipe model using webcam\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "      \n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render detections\n",
    "        draw_landmarks(image, results)               \n",
    "        \n",
    "        # Display frame on screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        # Exit / break out logic\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df986fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recollect and organize keypoints from the test\n",
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a654c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33 landmarks with 4 values (x, y, z, visibility)\n",
    "num_landmarks = len(landmarks)\n",
    "num_values = len(test)\n",
    "num_input_values = num_landmarks*num_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645bbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of what we would use as an input into our AI models\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3d007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Processes and organizes the keypoints detected from the pose estimation model \n",
    "    to be used as inputs for the exercise decoder models\n",
    "    \n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9476d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomar\\data\n"
     ]
    }
   ],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(os. getcwd(),'data') \n",
    "print(DATA_PATH)\n",
    "\n",
    "# make directory if it does not exist yet\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Actions/exercises that we try to detect\n",
    "actions = np.array(['curl', 'press', 'squat'])\n",
    "num_classes = len(actions)\n",
    "\n",
    "# How many videos worth of data\n",
    "no_sequences = 50\n",
    "\n",
    "# Videos are going to be this many frames in length\n",
    "sequence_length = FPS*1\n",
    "\n",
    "# Folder start\n",
    "# Change this to collect more data and not lose previously collected data\n",
    "start_folder = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f3cf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build folder paths\n",
    "for action in actions:     \n",
    "    for sequence in range(start_folder,no_sequences+start_folder):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))  \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e079a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors associated with each exercise (e.g., curls are denoted by blue, squats are denoted by orange, etc.)\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcbd0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Training Data\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    # Loop through actions\n",
    "    for idx, action in enumerate(actions):\n",
    "        # Loop through sequences (i.e., videos)\n",
    "        for sequence in range(start_folder, start_folder+no_sequences):\n",
    "            # Loop through video length (i.e, sequence length)\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # Make detection\n",
    "                image, results = mediapipe_detection(frame, pose)\n",
    "\n",
    "                # Extract landmarks\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Render detections\n",
    "                draw_landmarks(image, results) \n",
    "\n",
    "                # Apply visualization logic\n",
    "                if frame_num == 0: # If first frame in sequence, print that you're starting a new data collection and wait 500 ms\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # Export keypoints (sequence + pose landmarks)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('m'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52ffc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e664767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad7e343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and organize recorded training data\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):         \n",
    "            # LSTM input data\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)  \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0257941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 30, 132) (150, 3)\n"
     ]
    }
   ],
   "source": [
    "# Make sure first dimensions of arrays match\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20236938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 30, 132) (135, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e27f8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to be used during neural network training \n",
    "es_callback = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, verbose=0, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0, mode='min')\n",
    "chkpt_callback = ModelCheckpoint(filepath=DATA_PATH, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq=1)\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# some hyperparamters\n",
    "batch_size = 32\n",
    "max_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fad93fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-LSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16aedba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 128)           133632    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 256)           394240    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 749,955\n",
      "Trainable params: 749,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(sequence_length, num_input_values)))\n",
    "lstm.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "lstm.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "lstm.add(Dense(128, activation='relu'))\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57f6554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 4s 231ms/step - loss: 4.8459 - categorical_accuracy: 0.3214 - val_loss: 2.2373 - val_categorical_accuracy: 0.1739 - lr: 8.0000e-05\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 3.3910 - categorical_accuracy: 0.3036 - val_loss: 4.6706 - val_categorical_accuracy: 0.3913 - lr: 8.0000e-05\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 2.4016 - categorical_accuracy: 0.3839 - val_loss: 3.2025 - val_categorical_accuracy: 0.3913 - lr: 8.0000e-05\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 2.7711 - categorical_accuracy: 0.3482 - val_loss: 2.8920 - val_categorical_accuracy: 0.1304 - lr: 8.0000e-05\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 2.0788 - categorical_accuracy: 0.3393 - val_loss: 2.4577 - val_categorical_accuracy: 0.4348 - lr: 8.0000e-05\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 2.0415 - categorical_accuracy: 0.3214 - val_loss: 1.9949 - val_categorical_accuracy: 0.4348 - lr: 8.0000e-05\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.7434 - categorical_accuracy: 0.4196 - val_loss: 2.6746 - val_categorical_accuracy: 0.2174 - lr: 8.0000e-05\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.6082 - categorical_accuracy: 0.3214 - val_loss: 2.2806 - val_categorical_accuracy: 0.3913 - lr: 8.0000e-05\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 1.4448 - categorical_accuracy: 0.4018 - val_loss: 2.3414 - val_categorical_accuracy: 0.4348 - lr: 8.0000e-05\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.5511 - categorical_accuracy: 0.3839 - val_loss: 2.0987 - val_categorical_accuracy: 0.3913 - lr: 8.0000e-05\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 1.4655 - categorical_accuracy: 0.4018 - val_loss: 2.0558 - val_categorical_accuracy: 0.3478 - lr: 8.0000e-05\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 1.3433 - categorical_accuracy: 0.4286 - val_loss: 1.9648 - val_categorical_accuracy: 0.2174 - lr: 1.6000e-05\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 1.3085 - categorical_accuracy: 0.4732 - val_loss: 1.9902 - val_categorical_accuracy: 0.3478 - lr: 1.6000e-05\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 1.3406 - categorical_accuracy: 0.3839 - val_loss: 1.9560 - val_categorical_accuracy: 0.3478 - lr: 1.6000e-05\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.3047 - categorical_accuracy: 0.4286 - val_loss: 1.9661 - val_categorical_accuracy: 0.3043 - lr: 1.6000e-05\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 1.3247 - categorical_accuracy: 0.4286 - val_loss: 1.9733 - val_categorical_accuracy: 0.3478 - lr: 1.6000e-05\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 1.2963 - categorical_accuracy: 0.4196 - val_loss: 1.9480 - val_categorical_accuracy: 0.3913 - lr: 1.6000e-05\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.3180 - categorical_accuracy: 0.4732 - val_loss: 2.0014 - val_categorical_accuracy: 0.3913 - lr: 1.6000e-05\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 90ms/step - loss: 1.3213 - categorical_accuracy: 0.4554 - val_loss: 1.9840 - val_categorical_accuracy: 0.2174 - lr: 1.6000e-05\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 1.2956 - categorical_accuracy: 0.3929 - val_loss: 1.9798 - val_categorical_accuracy: 0.3043 - lr: 1.6000e-05\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.3225 - categorical_accuracy: 0.4107 - val_loss: 1.9762 - val_categorical_accuracy: 0.3478 - lr: 1.6000e-05\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 1.3011 - categorical_accuracy: 0.4107 - val_loss: 1.9522 - val_categorical_accuracy: 0.4348 - lr: 1.6000e-05\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 1.3132 - categorical_accuracy: 0.4196 - val_loss: 1.9704 - val_categorical_accuracy: 0.3478 - lr: 1.0000e-05\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.3284 - categorical_accuracy: 0.3750 - val_loss: 1.9586 - val_categorical_accuracy: 0.3478 - lr: 1.0000e-05\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 1.2958 - categorical_accuracy: 0.4643 - val_loss: 1.9700 - val_categorical_accuracy: 0.2174 - lr: 1.0000e-05\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 91ms/step - loss: 1.2963 - categorical_accuracy: 0.4375 - val_loss: 1.9860 - val_categorical_accuracy: 0.3478 - lr: 1.0000e-05\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.3115 - categorical_accuracy: 0.4018 - val_loss: 1.9520 - val_categorical_accuracy: 0.3913 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16501acc7c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ebdf8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b321aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a32d2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30, 132)]    0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 30, 512)      796672      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 512, 30)      0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512, 30)      930         ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " attention_vec (Permute)        (None, 30, 512)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " attention_mul (Multiply)       (None, 30, 512)      0           ['bidirectional[0][0]',          \n",
      "                                                                  'attention_vec[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 15360)        0           ['attention_mul[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          7864832     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            1539        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,663,973\n",
      "Trainable params: 8,663,973\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(sequence_length, num_input_values))\n",
    "\n",
    "# Bi-LSTM\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "\n",
    "# Attention\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output\n",
    "x = Dense(actions.shape[0], activation='softmax')(x)\n",
    "\n",
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "876af40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 4s 345ms/step - loss: 1.0953 - categorical_accuracy: 0.4018 - val_loss: 1.0938 - val_categorical_accuracy: 0.5652 - lr: 1.0000e-05\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 106ms/step - loss: 1.0904 - categorical_accuracy: 0.4732 - val_loss: 1.0898 - val_categorical_accuracy: 0.3043 - lr: 1.0000e-05\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 1.0802 - categorical_accuracy: 0.5804 - val_loss: 1.0833 - val_categorical_accuracy: 0.6522 - lr: 1.0000e-05\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 104ms/step - loss: 1.0724 - categorical_accuracy: 0.6161 - val_loss: 1.0773 - val_categorical_accuracy: 0.6522 - lr: 1.0000e-05\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 1.0618 - categorical_accuracy: 0.6696 - val_loss: 1.0699 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 108ms/step - loss: 1.0533 - categorical_accuracy: 0.6250 - val_loss: 1.0626 - val_categorical_accuracy: 0.4783 - lr: 1.0000e-05\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 1.0405 - categorical_accuracy: 0.6607 - val_loss: 1.0521 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 95ms/step - loss: 1.0285 - categorical_accuracy: 0.7321 - val_loss: 1.0412 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 116ms/step - loss: 1.0142 - categorical_accuracy: 0.7232 - val_loss: 1.0292 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.9971 - categorical_accuracy: 0.7589 - val_loss: 1.0154 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 1s 162ms/step - loss: 0.9782 - categorical_accuracy: 0.7768 - val_loss: 1.0018 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.9663 - categorical_accuracy: 0.8036 - val_loss: 0.9862 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.9449 - categorical_accuracy: 0.7768 - val_loss: 0.9680 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.9220 - categorical_accuracy: 0.8125 - val_loss: 0.9496 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 1s 163ms/step - loss: 0.9032 - categorical_accuracy: 0.7857 - val_loss: 0.9290 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.8787 - categorical_accuracy: 0.7768 - val_loss: 0.9092 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 0.8564 - categorical_accuracy: 0.8036 - val_loss: 0.8884 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.8221 - categorical_accuracy: 0.8036 - val_loss: 0.8671 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.7957 - categorical_accuracy: 0.7768 - val_loss: 0.8418 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 0.7707 - categorical_accuracy: 0.8125 - val_loss: 0.8157 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 1s 132ms/step - loss: 0.7367 - categorical_accuracy: 0.8036 - val_loss: 0.7884 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 1s 166ms/step - loss: 0.7194 - categorical_accuracy: 0.8036 - val_loss: 0.7643 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.6868 - categorical_accuracy: 0.8214 - val_loss: 0.7419 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.6588 - categorical_accuracy: 0.8125 - val_loss: 0.7177 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.6360 - categorical_accuracy: 0.8125 - val_loss: 0.6939 - val_categorical_accuracy: 0.7391 - lr: 1.0000e-05\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.6190 - categorical_accuracy: 0.7946 - val_loss: 0.6732 - val_categorical_accuracy: 0.7391 - lr: 1.0000e-05\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.6009 - categorical_accuracy: 0.8304 - val_loss: 0.6539 - val_categorical_accuracy: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.5691 - categorical_accuracy: 0.8393 - val_loss: 0.6354 - val_categorical_accuracy: 0.7391 - lr: 1.0000e-05\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.5440 - categorical_accuracy: 0.8393 - val_loss: 0.6177 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.5376 - categorical_accuracy: 0.8214 - val_loss: 0.6007 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 1s 155ms/step - loss: 0.5182 - categorical_accuracy: 0.8393 - val_loss: 0.5837 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.5042 - categorical_accuracy: 0.8393 - val_loss: 0.5687 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.5025 - categorical_accuracy: 0.8839 - val_loss: 0.5543 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 0.4805 - categorical_accuracy: 0.8929 - val_loss: 0.5416 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.4740 - categorical_accuracy: 0.8661 - val_loss: 0.5302 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.4599 - categorical_accuracy: 0.8571 - val_loss: 0.5172 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.4534 - categorical_accuracy: 0.8304 - val_loss: 0.5037 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.4417 - categorical_accuracy: 0.8661 - val_loss: 0.4908 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.4262 - categorical_accuracy: 0.8839 - val_loss: 0.4845 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.4143 - categorical_accuracy: 0.8750 - val_loss: 0.4766 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.4102 - categorical_accuracy: 0.8839 - val_loss: 0.4632 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.4005 - categorical_accuracy: 0.8750 - val_loss: 0.4498 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.4011 - categorical_accuracy: 0.8571 - val_loss: 0.4396 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 0.4079 - categorical_accuracy: 0.8571 - val_loss: 0.4272 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.3915 - categorical_accuracy: 0.8482 - val_loss: 0.4260 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.3778 - categorical_accuracy: 0.8839 - val_loss: 0.4222 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.3730 - categorical_accuracy: 0.8571 - val_loss: 0.4135 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.3707 - categorical_accuracy: 0.8571 - val_loss: 0.4105 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.3598 - categorical_accuracy: 0.8750 - val_loss: 0.4104 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.3580 - categorical_accuracy: 0.8571 - val_loss: 0.4009 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.3597 - categorical_accuracy: 0.8929 - val_loss: 0.3815 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.3550 - categorical_accuracy: 0.8661 - val_loss: 0.3706 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.3491 - categorical_accuracy: 0.8839 - val_loss: 0.3800 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.3488 - categorical_accuracy: 0.8661 - val_loss: 0.3984 - val_categorical_accuracy: 0.7826 - lr: 1.0000e-05\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.3429 - categorical_accuracy: 0.8929 - val_loss: 0.3773 - val_categorical_accuracy: 0.8261 - lr: 1.0000e-05\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.3493 - categorical_accuracy: 0.8750 - val_loss: 0.3622 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.3338 - categorical_accuracy: 0.8571 - val_loss: 0.3565 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.3259 - categorical_accuracy: 0.8661 - val_loss: 0.3450 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.3417 - categorical_accuracy: 0.8661 - val_loss: 0.3409 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.3267 - categorical_accuracy: 0.8929 - val_loss: 0.3458 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.3346 - categorical_accuracy: 0.8750 - val_loss: 0.3464 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 0.3080 - categorical_accuracy: 0.8750 - val_loss: 0.3506 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.3242 - categorical_accuracy: 0.8839 - val_loss: 0.3488 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.3304 - categorical_accuracy: 0.8750 - val_loss: 0.3339 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.3075 - categorical_accuracy: 0.8839 - val_loss: 0.3338 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.3157 - categorical_accuracy: 0.8839 - val_loss: 0.3250 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.3205 - categorical_accuracy: 0.8750 - val_loss: 0.3267 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 0.3023 - categorical_accuracy: 0.8661 - val_loss: 0.3288 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.3109 - categorical_accuracy: 0.8929 - val_loss: 0.3345 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 0.3076 - categorical_accuracy: 0.8929 - val_loss: 0.3367 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.3227 - categorical_accuracy: 0.8750 - val_loss: 0.3277 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.3200 - categorical_accuracy: 0.8571 - val_loss: 0.3042 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.3039 - categorical_accuracy: 0.8661 - val_loss: 0.2995 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.2895 - categorical_accuracy: 0.8929 - val_loss: 0.3075 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2935 - categorical_accuracy: 0.9018 - val_loss: 0.3092 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.2895 - categorical_accuracy: 0.9018 - val_loss: 0.3013 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.2982 - categorical_accuracy: 0.9018 - val_loss: 0.3139 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.2788 - categorical_accuracy: 0.9018 - val_loss: 0.3200 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.3003 - categorical_accuracy: 0.8839 - val_loss: 0.3142 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.2979 - categorical_accuracy: 0.8661 - val_loss: 0.2935 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.2859 - categorical_accuracy: 0.8839 - val_loss: 0.2865 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.2670 - categorical_accuracy: 0.8929 - val_loss: 0.3093 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 0.3068 - categorical_accuracy: 0.8839 - val_loss: 0.3135 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 0.2794 - categorical_accuracy: 0.9107 - val_loss: 0.2946 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2776 - categorical_accuracy: 0.9018 - val_loss: 0.2823 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.2681 - categorical_accuracy: 0.8929 - val_loss: 0.2865 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.2876 - categorical_accuracy: 0.8929 - val_loss: 0.2991 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.2706 - categorical_accuracy: 0.9018 - val_loss: 0.2956 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 0.2830 - categorical_accuracy: 0.8839 - val_loss: 0.2951 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.2656 - categorical_accuracy: 0.8929 - val_loss: 0.2885 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.2766 - categorical_accuracy: 0.8929 - val_loss: 0.2786 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.2657 - categorical_accuracy: 0.8839 - val_loss: 0.2746 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 0.2600 - categorical_accuracy: 0.8929 - val_loss: 0.2890 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2771 - categorical_accuracy: 0.8929 - val_loss: 0.2939 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.2694 - categorical_accuracy: 0.8929 - val_loss: 0.2867 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2563 - categorical_accuracy: 0.9018 - val_loss: 0.2788 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.2797 - categorical_accuracy: 0.8929 - val_loss: 0.2796 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.2667 - categorical_accuracy: 0.9107 - val_loss: 0.2759 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 1s 132ms/step - loss: 0.2617 - categorical_accuracy: 0.8929 - val_loss: 0.2915 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.2675 - categorical_accuracy: 0.9107 - val_loss: 0.2796 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.2644 - categorical_accuracy: 0.9018 - val_loss: 0.2650 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2788 - categorical_accuracy: 0.8750 - val_loss: 0.2695 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 0.2666 - categorical_accuracy: 0.8929 - val_loss: 0.2812 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.2938 - categorical_accuracy: 0.9018 - val_loss: 0.2848 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.2639 - categorical_accuracy: 0.9018 - val_loss: 0.2750 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.2571 - categorical_accuracy: 0.8839 - val_loss: 0.2620 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.2556 - categorical_accuracy: 0.8929 - val_loss: 0.2668 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2609 - categorical_accuracy: 0.9107 - val_loss: 0.2824 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.2635 - categorical_accuracy: 0.8839 - val_loss: 0.2636 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.2601 - categorical_accuracy: 0.8929 - val_loss: 0.2570 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2549 - categorical_accuracy: 0.8929 - val_loss: 0.2606 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.2616 - categorical_accuracy: 0.8929 - val_loss: 0.2718 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.2526 - categorical_accuracy: 0.8929 - val_loss: 0.2825 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2651 - categorical_accuracy: 0.9018 - val_loss: 0.2692 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2481 - categorical_accuracy: 0.9107 - val_loss: 0.2596 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.2597 - categorical_accuracy: 0.8839 - val_loss: 0.2540 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 117/500\n",
      "4/4 [==============================] - 1s 138ms/step - loss: 0.2594 - categorical_accuracy: 0.8750 - val_loss: 0.2574 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.2566 - categorical_accuracy: 0.8929 - val_loss: 0.2714 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2530 - categorical_accuracy: 0.9018 - val_loss: 0.2715 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.2541 - categorical_accuracy: 0.9018 - val_loss: 0.2607 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 1s 141ms/step - loss: 0.2518 - categorical_accuracy: 0.8929 - val_loss: 0.2506 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.2352 - categorical_accuracy: 0.9018 - val_loss: 0.2568 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.2497 - categorical_accuracy: 0.9107 - val_loss: 0.2783 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.2559 - categorical_accuracy: 0.9018 - val_loss: 0.2935 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.2462 - categorical_accuracy: 0.9107 - val_loss: 0.2705 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.2428 - categorical_accuracy: 0.9018 - val_loss: 0.2506 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 0.2486 - categorical_accuracy: 0.8929 - val_loss: 0.2376 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.2588 - categorical_accuracy: 0.8839 - val_loss: 0.2432 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.2511 - categorical_accuracy: 0.9107 - val_loss: 0.2719 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 1s 153ms/step - loss: 0.2409 - categorical_accuracy: 0.9018 - val_loss: 0.2595 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 0.2514 - categorical_accuracy: 0.9018 - val_loss: 0.2414 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.2431 - categorical_accuracy: 0.8661 - val_loss: 0.2387 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 1s 158ms/step - loss: 0.2467 - categorical_accuracy: 0.9107 - val_loss: 0.2616 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.2398 - categorical_accuracy: 0.9107 - val_loss: 0.2781 - val_categorical_accuracy: 0.8696 - lr: 1.0000e-05\n",
      "Epoch 135/500\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 0.2620 - categorical_accuracy: 0.8929 - val_loss: 0.2631 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.2405 - categorical_accuracy: 0.9018 - val_loss: 0.2468 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.2394 - categorical_accuracy: 0.9018 - val_loss: 0.2462 - val_categorical_accuracy: 0.9130 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16500b95ab0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttnLSTM.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b211b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model map\n",
    "models = {\n",
    "    'LSTM': lstm, \n",
    "    'LSTM_Attention_128HUs': AttnLSTM, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d865a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81769410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model rebuild before doing this\n",
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cb0719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    res = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da016288",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "187b859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM confusion matrix: \r\n",
      "[[[7 3]\n",
      "  [3 2]]\n",
      "\n",
      " [[7 2]\n",
      "  [6 0]]\n",
      "\n",
      " [[5 6]\n",
      "  [2 2]]]\n",
      "LSTM_Attention_128HUs confusion matrix: \r\n",
      "[[[ 8  2]\n",
      "  [ 0  5]]\n",
      "\n",
      " [[ 9  0]\n",
      "  [ 2  4]]\n",
      "\n",
      " [[11  0]\n",
      "  [ 0  4]]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['confusion matrix'] = confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71f04e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM classification accuracy = 26.667%\n",
      "LSTM_Attention_128HUs classification accuracy = 86.667%\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Model accuracy\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['accuracy'] = classification_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49b0ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM weighted average precision = 0.2\n",
      "LSTM weighted average recall = 0.267\n",
      "LSTM weighted average f1-score = 0.222\n",
      "\n",
      "LSTM_Attention_128HUs weighted average precision = 0.905\n",
      "LSTM_Attention_128HUs weighted average recall = 0.867\n",
      "LSTM_Attention_128HUs weighted average f1-score = 0.864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Precision, recall, and f1 score\n",
    "    report = classification_report(ytrue, yhat, target_names=actions, output_dict=True)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "   \n",
    "    print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\n\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42e5a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "621af785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    \"\"\"\n",
    "    Computes 3D joint angle inferred by 3 keypoints and their relative positions to one another\n",
    "    \n",
    "    \"\"\"\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38839f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(landmarks, mp_pose, side, joint):\n",
    "    \"\"\"\n",
    "    Retrieves x and y coordinates of a particular keypoint from the pose estimation model\n",
    "         \n",
    "     Args:\n",
    "         landmarks: processed keypoints from the pose estimation model\n",
    "         mp_pose: Mediapipe pose estimation model\n",
    "         side: 'left' or 'right'. Denotes the side of the body of the landmark of interest.\n",
    "         joint: 'shoulder', 'elbow', 'wrist', 'hip', 'knee', or 'ankle'. Denotes which body joint is associated with the landmark of interest.\n",
    "    \n",
    "    \"\"\"\n",
    "    coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "    x_coord_val = landmarks[coord.value].x\n",
    "    y_coord_val = landmarks[coord.value].y\n",
    "    return [x_coord_val, y_coord_val]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5eeca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_joint_angle(image, angle, joint):\n",
    "    \"\"\"\n",
    "    Displays the joint angle value near the joint within the image frame\n",
    "    \n",
    "    \"\"\"\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                   tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1af746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    \"\"\"\n",
    "    Counts repetitions of each exercise. Global count and stage (i.e., state) variables are updated within this function.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    global curl_counter, press_counter, squat_counter, curl_stage, press_stage, squat_stage\n",
    "    \n",
    "    if current_action == 'curl':\n",
    "        # Get coords\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        \n",
    "        # calculate elbow angle\n",
    "        angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        # curl counter logic\n",
    "        if angle < 30:\n",
    "            curl_stage = \"up\" \n",
    "        if angle > 140 and curl_stage =='up':\n",
    "            curl_stage=\"down\"  \n",
    "            curl_counter +=1\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "            \n",
    "        # Viz joint angle\n",
    "        viz_joint_angle(image, angle, elbow)\n",
    "        \n",
    "    elif current_action == 'press':\n",
    "        \n",
    "        # Get coords\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "\n",
    "        # Calculate elbow angle\n",
    "        elbow_angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        \n",
    "        # Compute distances between joints\n",
    "        shoulder2elbow_dist = abs(math.dist(shoulder,elbow))\n",
    "        shoulder2wrist_dist = abs(math.dist(shoulder,wrist))\n",
    "        \n",
    "        # Press counter logic\n",
    "        if (elbow_angle > 130) and (shoulder2elbow_dist < shoulder2wrist_dist):\n",
    "            press_stage = \"up\"\n",
    "        if (elbow_angle < 50) and (shoulder2elbow_dist > shoulder2wrist_dist) and (press_stage =='up'):\n",
    "            press_stage='down'\n",
    "            press_counter += 1\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "            \n",
    "        # Viz joint angle\n",
    "        viz_joint_angle(image, elbow_angle, elbow)\n",
    "        \n",
    "    elif current_action == 'squat':\n",
    "        # Get coords\n",
    "        # left side\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        # right side\n",
    "        right_shoulder = get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_hip = get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        # Calculate knee angles\n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        \n",
    "        # Calculate hip angles\n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "        \n",
    "        # Squat counter logic\n",
    "        thr = 165\n",
    "        if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "            squat_stage = \"down\"\n",
    "        if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (squat_stage =='down'):\n",
    "            squat_stage='up'\n",
    "            squat_counter += 1\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "            \n",
    "        # Viz joint angles\n",
    "        viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "        viz_joint_angle(image, left_hip_angle, left_hip)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10c5efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    \"\"\"\n",
    "    This function displays the model prediction probability distribution over the set of exercise classes\n",
    "    as a horizontal bar graph\n",
    "    \n",
    "    \"\"\"\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d716475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "predictions = []\n",
    "res = []\n",
    "threshold = 0.5 # minimum confidence to classify as an action/exercise\n",
    "current_action = ''\n",
    "\n",
    "# Rep counter logic variables\n",
    "curl_counter = 0\n",
    "press_counter = 0\n",
    "squat_counter = 0\n",
    "curl_stage = None\n",
    "press_stage = None\n",
    "squat_stage = None\n",
    "\n",
    "# Camera object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Video writer object that saves a video of the real time test\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') # video compression format\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # webcam video frame height\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # webcam video frame width\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS)) # webcam video fram rate \n",
    "\n",
    "video_name = os.path.join(os.getcwd(),f\"{model_name}_real_time_test.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH,HEIGHT))\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(res))\n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "        #3. Viz logic\n",
    "            # Erase current action variable if no probability is above threshold\n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "            # Count reps\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                count_reps(\n",
    "                    image, current_action, landmarks, mp_pose)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Display graphical information\n",
    "            cv2.rectangle(image, (0,0), (640, 40), colors[np.argmax(res)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(curl_counter), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'press ' + str(press_counter), (240,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'squat ' + str(squat_counter), (490,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "         \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        # Write to video file\n",
    "        if ret == True:\n",
    "            out.write(image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b6b9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c23bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f66a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa485d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
